/*
 * Copyright Â© 2009 Nokia Corporation
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
 * DEALINGS IN THE SOFTWARE.
 */

/*
 * WARNING: this file was autogenerated from a macro template. Sources are
 * maintained as part of pixman library: http://cgit.freedesktop.org/pixman
 */

/* Prevent the stack from becoming executable for no reason... */
#if defined(__linux__) && defined(__ELF__)
.section .note.GNU-stack,"",%progbits
#endif

#ifdef __ARM_ARCH_7A__

.text
.fpu neon
.arch armv7a
.syntax unified

#ifdef __ELF__
.hidden gdk_composite_src_0888_0565_rev_asm_neon
.type gdk_composite_src_0888_0565_rev_asm_neon, %function
#endif
.global gdk_composite_src_0888_0565_rev_asm_neon
gdk_composite_src_0888_0565_rev_asm_neon:
	push	{r4, r5, r6, r7, r8, r9, sl, fp, ip, lr}
	ldr	r4, [sp, #40]
	ldr	r5, [sp, #44]
	mov	r6, r2
	sub	r5, r5, r0
	sub	r5, r5, r0, lsl #1
	subs	r1, r1, #1
	mov	sl, r0
	blt	0f
	cmp	r0, #16
	blt	1f
16:	tst	r6, #15
	beq	2f
	tst	r6, #2
	beq	3f
	vld3.8	{d0[1],d1[1],d2[1]}, [r4]!
	add	r6, r6, #2
	sub	r0, r0, #1
3:	tst	r6, #4
	beq	4f
	vld3.8	{d0[2],d1[2],d2[2]}, [r4]!
	vld3.8	{d0[3],d1[3],d2[3]}, [r4]!
	add	r6, r6, #4
	sub	r0, r0, #2
4:	tst	r6, #8
	beq	5f
	vld3.8	{d0[4],d1[4],d2[4]}, [r4]!
	vld3.8	{d0[5],d1[5],d2[5]}, [r4]!
	vld3.8	{d0[6],d1[6],d2[6]}, [r4]!
	vld3.8	{d0[7],d1[7],d2[7]}, [r4]!
	add	r6, r6, #8
	sub	r0, r0, #4
5:	vshll.i8	q8, d1, #8
	vshll.i8	q9, d2, #8
	pld	[r4, #192]
	vshll.i8	q14, d0, #8
	vsri.16	q14, q8, #5
	vsri.16	q14, q9, #11
	tst	r2, #2
	beq	6f
	vst1.16	{d28[1]}, [r2]!
6:	tst	r2, #4
	beq	7f
	vst1.16	{d28[2]}, [r2]!
	vst1.16	{d28[3]}, [r2]!
7:	tst	r2, #8
	beq	2f
	vst1.16	{d29}, [r2, :64]!
2:	vld3.8	{d0-d2}, [r4]!
	vshll.i8	q8, d1, #8
	vshll.i8	q9, d2, #8
	pld	[r4, #192]
	subs	r0, r0, #16
	blt	8f
9:	vshll.i8	q14, d0, #8
	vld3.8	{d0-d2}, [r4]!
	vsri.16	q14, q8, #5
	vsri.16	q14, q9, #11
	vshll.i8	q8, d1, #8
	vst1.16	{d28-d29}, [r2, :128]!
	vshll.i8	q9, d2, #8
	pld	[r4, #192]
	subs	r0, r0, #8
	bge	9b
8:	vshll.i8	q14, d0, #8
	vsri.16	q14, q8, #5
	vsri.16	q14, q9, #11
	vst1.16	{d28-d29}, [r2, :128]!
	tst	r0, #7
	beq	10f
	tst	r0, #4
	beq	11f
	vld3.8	{d0[4],d1[4],d2[4]}, [r4]!
	vld3.8	{d0[5],d1[5],d2[5]}, [r4]!
	vld3.8	{d0[6],d1[6],d2[6]}, [r4]!
	vld3.8	{d0[7],d1[7],d2[7]}, [r4]!
11:	tst	r0, #2
	beq	12f
	vld3.8	{d0[2],d1[2],d2[2]}, [r4]!
	vld3.8	{d0[3],d1[3],d2[3]}, [r4]!
12:	tst	r0, #1
	beq	13f
	vld3.8	{d0[1],d1[1],d2[1]}, [r4]!
13:	vshll.i8	q8, d1, #8
	vshll.i8	q9, d2, #8
	pld	[r4, #192]
	vshll.i8	q14, d0, #8
	vsri.16	q14, q8, #5
	vsri.16	q14, q9, #11
	tst	r0, #4
	beq	14f
	vst1.16	{d29}, [r2, :64]!
14:	tst	r0, #2
	beq	15f
	vst1.16	{d28[2]}, [r2]!
	vst1.16	{d28[3]}, [r2]!
15:	tst	r0, #1
	beq	10f
	vst1.16	{d28[1]}, [r2]!
10:	mov	r0, sl
	add	r2, r2, r3, lsl #1
	add	r4, r4, r5
	sub	r2, r2, r0, lsl #1
	subs	r1, r1, #1
	mov	r6, r2
	bge	16b
	pop	{r4, r5, r6, r7, r8, r9, sl, fp, ip, pc}
1:	tst	r0, #8
	beq	17f
	vld3.8	{d0-d2}, [r4]!
	vshll.i8	q8, d1, #8
	vshll.i8	q9, d2, #8
	vshll.i8	q14, d0, #8
	vsri.16	q14, q8, #5
	vsri.16	q14, q9, #11
	vst1.16	{d28-d29}, [r2]!
17:	tst	r0, #7
	beq	18f
	tst	r0, #4
	beq	19f
	vld3.8	{d0[4],d1[4],d2[4]}, [r4]!
	vld3.8	{d0[5],d1[5],d2[5]}, [r4]!
	vld3.8	{d0[6],d1[6],d2[6]}, [r4]!
	vld3.8	{d0[7],d1[7],d2[7]}, [r4]!
19:	tst	r0, #2
	beq	20f
	vld3.8	{d0[2],d1[2],d2[2]}, [r4]!
	vld3.8	{d0[3],d1[3],d2[3]}, [r4]!
20:	tst	r0, #1
	beq	21f
	vld3.8	{d0[1],d1[1],d2[1]}, [r4]!
21:	vshll.i8	q8, d1, #8
	vshll.i8	q9, d2, #8
	vshll.i8	q14, d0, #8
	vsri.16	q14, q8, #5
	vsri.16	q14, q9, #11
	tst	r0, #4
	beq	22f
	vst1.16	{d29}, [r2]!
22:	tst	r0, #2
	beq	23f
	vst1.16	{d28[2]}, [r2]!
	vst1.16	{d28[3]}, [r2]!
23:	tst	r0, #1
	beq	18f
	vst1.16	{d28[1]}, [r2]!
18:	mov	r0, sl
	add	r2, r2, r3, lsl #1
	add	r4, r4, r5
	sub	r2, r2, r0, lsl #1
	subs	r1, r1, #1
	mov	r6, r2
	bge	1b
0:	pop	{r4, r5, r6, r7, r8, r9, sl, fp, ip, pc}

#ifdef __ELF__
.hidden gdk_composite_src_0888_8888_rev_asm_neon
.type gdk_composite_src_0888_8888_rev_asm_neon, %function
#endif
.global gdk_composite_src_0888_8888_rev_asm_neon
gdk_composite_src_0888_8888_rev_asm_neon:
	push	{r4, r5, r6, r7, r8, r9, sl, fp, ip, lr}
	ldr	r4, [sp, #40]
	ldr	r5, [sp, #44]
	mov	r6, r2
	sub	r5, r5, r0
	sub	r5, r5, r0, lsl #1
	veor	d3, d3, d3
	subs	r1, r1, #1
	mov	sl, r0
	blt	0f
	cmp	r0, #16
	blt	1f
15:	tst	r6, #15
	beq	2f
	tst	r6, #4
	beq	3f
	vld3.8	{d0[1],d1[1],d2[1]}, [r4]!
	add	r6, r6, #4
	sub	r0, r0, #1
3:	tst	r6, #8
	beq	4f
	vld3.8	{d0[2],d1[2],d2[2]}, [r4]!
	vld3.8	{d0[3],d1[3],d2[3]}, [r4]!
	add	r6, r6, #8
	sub	r0, r0, #2
4:	vld3.8	{d0[4],d1[4],d2[4]}, [r4]!
	vld3.8	{d0[5],d1[5],d2[5]}, [r4]!
	vld3.8	{d0[6],d1[6],d2[6]}, [r4]!
	vld3.8	{d0[7],d1[7],d2[7]}, [r4]!
	add	r6, r6, #16
	sub	r0, r0, #4
	vswp	d0, d2
	pld	[r4, #192]
	vzip.8	d0, d2
	vzip.8	d1, d3
	vzip.8	d2, d3
	vzip.8	d0, d1
	tst	r2, #4
	beq	5f
	vst1.32	{d0[1]}, [r2]!
5:	tst	r2, #8
	beq	6f
	vst1.32	{d1}, [r2, :64]!
6:	vst1.32	{d2-d3}, [r2, :128]!
2:	vld3.8	{d0-d2}, [r4]!
	vswp	d0, d2
	pld	[r4, #192]
	subs	r0, r0, #16
	blt	7f
8:	vst4.8	{d0-d3}, [r2]!
	vld3.8	{d0-d2}, [r4]!
	vswp	d0, d2
	pld	[r4, #192]
	subs	r0, r0, #8
	bge	8b
7:	vst4.8	{d0-d3}, [r2, :128]!
	tst	r0, #7
	beq	9f
	tst	r0, #4
	beq	10f
	vld3.8	{d0[4],d1[4],d2[4]}, [r4]!
	vld3.8	{d0[5],d1[5],d2[5]}, [r4]!
	vld3.8	{d0[6],d1[6],d2[6]}, [r4]!
	vld3.8	{d0[7],d1[7],d2[7]}, [r4]!
10:	tst	r0, #2
	beq	11f
	vld3.8	{d0[2],d1[2],d2[2]}, [r4]!
	vld3.8	{d0[3],d1[3],d2[3]}, [r4]!
11:	tst	r0, #1
	beq	12f
	vld3.8	{d0[1],d1[1],d2[1]}, [r4]!
12:	vswp	d0, d2
	pld	[r4, #192]
	vzip.8	d0, d2
	vzip.8	d1, d3
	vzip.8	d2, d3
	vzip.8	d0, d1
	tst	r0, #4
	beq	13f
	vst1.32	{d2-d3}, [r2, :128]!
13:	tst	r0, #2
	beq	14f
	vst1.32	{d1}, [r2, :64]!
14:	tst	r0, #1
	beq	9f
	vst1.32	{d0[1]}, [r2]!
9:	mov	r0, sl
	add	r2, r2, r3, lsl #2
	add	r4, r4, r5
	sub	r2, r2, r0, lsl #2
	subs	r1, r1, #1
	mov	r6, r2
	bge	15b
	pop	{r4, r5, r6, r7, r8, r9, sl, fp, ip, pc}
1:	tst	r0, #8
	beq	16f
	vld3.8	{d0-d2}, [r4]!
	vswp	d0, d2
	vst4.8	{d0-d3}, [r2]!
16:	tst	r0, #7
	beq	17f
	tst	r0, #4
	beq	18f
	vld3.8	{d0[4],d1[4],d2[4]}, [r4]!
	vld3.8	{d0[5],d1[5],d2[5]}, [r4]!
	vld3.8	{d0[6],d1[6],d2[6]}, [r4]!
	vld3.8	{d0[7],d1[7],d2[7]}, [r4]!
18:	tst	r0, #2
	beq	19f
	vld3.8	{d0[2],d1[2],d2[2]}, [r4]!
	vld3.8	{d0[3],d1[3],d2[3]}, [r4]!
19:	tst	r0, #1
	beq	20f
	vld3.8	{d0[1],d1[1],d2[1]}, [r4]!
20:	vswp	d0, d2
	vzip.8	d0, d2
	vzip.8	d1, d3
	vzip.8	d2, d3
	vzip.8	d0, d1
	tst	r0, #4
	beq	21f
	vst1.32	{d2-d3}, [r2]!
21:	tst	r0, #2
	beq	22f
	vst1.32	{d1}, [r2]!
22:	tst	r0, #1
	beq	17f
	vst1.32	{d0[1]}, [r2]!
17:	mov	r0, sl
	add	r2, r2, r3, lsl #2
	add	r4, r4, r5
	sub	r2, r2, r0, lsl #2
	subs	r1, r1, #1
	mov	r6, r2
	bge	1b
0:	pop	{r4, r5, r6, r7, r8, r9, sl, fp, ip, pc}

#ifdef __ELF__
.hidden gdk_composite_src_pixbuf_8888_asm_neon
.type gdk_composite_src_pixbuf_8888_asm_neon, %function
#endif
.global gdk_composite_src_pixbuf_8888_asm_neon
gdk_composite_src_pixbuf_8888_asm_neon:
	push	{r4, r5, r6, r7, r8, r9, sl, fp, ip, lr}
	ldr	r4, [sp, #40]
	mov	sl, #0
	ldr	r5, [sp, #44]
	mov	r6, r2
	mov	fp, r4
	mov	ip, r6
	mov	lr, r7
	lsl	r9, r1, #4
	sub	r9, r9, #6
	subs	r1, r1, #1
	mov	r7, r0
	blt	0f
	cmp	r0, #16
	blt	1f
15:	tst	r6, #15
	beq	2f
	tst	r6, #4
	beq	3f
	vld1.32	{d0[1]}, [r4]!
	add	r6, r6, #4
	add	sl, sl, #1
	sub	r0, r0, #1
3:	tst	r6, #8
	beq	4f
	vld1.32	{d1}, [r4]!
	add	r6, r6, #8
	add	sl, sl, #2
	sub	r0, r0, #2
4:	vld1.32	{d2-d3}, [r4]!
	add	r6, r6, #16
	add	sl, sl, #4
	sub	r0, r0, #4
	vuzp.8	d0, d1
	vuzp.8	d2, d3
	vuzp.8	d1, d3
	vuzp.8	d0, d2
	vmull.u8	q8, d3, d0
	vmull.u8	q9, d3, d1
	vmull.u8	q10, d3, d2
	tst	r9, #15
	addne	sl, sl, #8
	subne	r9, r9, #1
	cmp	sl, r7
	pld	[fp, sl, lsl #2]
	subge	sl, sl, r7
	subsge	r9, r9, #16
	ldrbge	r8, [fp, r5, lsl #2]!
	vrshr.u16	q11, q8, #8
	vswp	d3, d31
	vrshr.u16	q12, q9, #8
	vrshr.u16	q13, q10, #8
	vraddhn.i16	d30, q11, q8
	vraddhn.i16	d29, q12, q9
	vraddhn.i16	d28, q13, q10
	vzip.8	d28, d30
	vzip.8	d29, d31
	vzip.8	d30, d31
	vzip.8	d28, d29
	tst	r2, #4
	beq	5f
	vst1.32	{d28[1]}, [r2]!
5:	tst	r2, #8
	beq	6f
	vst1.32	{d29}, [r2, :64]!
6:	vst1.32	{d30-d31}, [r2, :128]!
2:	vld4.8	{d0-d3}, [r4]!
	add	sl, sl, #8
	vmull.u8	q8, d3, d0
	vmull.u8	q9, d3, d1
	vmull.u8	q10, d3, d2
	tst	r9, #15
	addne	sl, sl, #8
	subne	r9, r9, #1
	cmp	sl, r7
	pld	[fp, sl, lsl #2]
	subge	sl, sl, r7
	subsge	r9, r9, #16
	ldrbge	r8, [fp, r5, lsl #2]!
	subs	r0, r0, #16
	blt	7f
8:	vrshr.u16	q11, q8, #8
	vswp	d3, d31
	vrshr.u16	q12, q9, #8
	vrshr.u16	q13, q10, #8
	vld4.8	{d0-d3}, [r4]!
	vraddhn.i16	d30, q11, q8
	add	sl, sl, #8
	tst	r9, #15
	addne	sl, sl, #8
	subne	r9, r9, #1
	vraddhn.i16	d29, q12, q9
	vraddhn.i16	d28, q13, q10
	vmull.u8	q8, d3, d0
	vmull.u8	q9, d3, d1
	vmull.u8	q10, d3, d2
	vst4.8	{d28-d31}, [r2, :128]!
	cmp	sl, r7
	pld	[fp, sl, lsl #2]
	subge	sl, sl, r7
	subsge	r9, r9, #16
	ldrbge	r8, [fp, r5, lsl #2]!
	subs	r0, r0, #8
	bge	8b
7:	vrshr.u16	q11, q8, #8
	vswp	d3, d31
	vrshr.u16	q12, q9, #8
	vrshr.u16	q13, q10, #8
	vraddhn.i16	d30, q11, q8
	vraddhn.i16	d29, q12, q9
	vraddhn.i16	d28, q13, q10
	vst4.8	{d28-d31}, [r2, :128]!
	tst	r0, #7
	beq	9f
	tst	r0, #4
	beq	10f
	vld1.32	{d2-d3}, [r4]!
	add	sl, sl, #4
10:	tst	r0, #2
	beq	11f
	vld1.32	{d1}, [r4]!
	add	sl, sl, #2
11:	tst	r0, #1
	beq	12f
	vld1.32	{d0[1]}, [r4]!
	add	sl, sl, #1
12:	vuzp.8	d0, d1
	vuzp.8	d2, d3
	vuzp.8	d1, d3
	vuzp.8	d0, d2
	vmull.u8	q8, d3, d0
	vmull.u8	q9, d3, d1
	vmull.u8	q10, d3, d2
	tst	r9, #15
	addne	sl, sl, #8
	subne	r9, r9, #1
	cmp	sl, r7
	pld	[fp, sl, lsl #2]
	subge	sl, sl, r7
	subsge	r9, r9, #16
	ldrbge	r8, [fp, r5, lsl #2]!
	vrshr.u16	q11, q8, #8
	vswp	d3, d31
	vrshr.u16	q12, q9, #8
	vrshr.u16	q13, q10, #8
	vraddhn.i16	d30, q11, q8
	vraddhn.i16	d29, q12, q9
	vraddhn.i16	d28, q13, q10
	vzip.8	d28, d30
	vzip.8	d29, d31
	vzip.8	d30, d31
	vzip.8	d28, d29
	tst	r0, #4
	beq	13f
	vst1.32	{d30-d31}, [r2, :128]!
13:	tst	r0, #2
	beq	14f
	vst1.32	{d29}, [r2, :64]!
14:	tst	r0, #1
	beq	9f
	vst1.32	{d28[1]}, [r2]!
9:	mov	r0, r7
	add	r2, r2, r3, lsl #2
	add	r4, r4, r5, lsl #2
	sub	r2, r2, r0, lsl #2
	sub	r4, r4, r0, lsl #2
	subs	r1, r1, #1
	mov	r6, r2
	bge	15b
	pop	{r4, r5, r6, r7, r8, r9, sl, fp, ip, pc}
1:	tst	r0, #8
	beq	16f
	vld4.8	{d0-d3}, [r4]!
	vmull.u8	q8, d3, d0
	vmull.u8	q9, d3, d1
	vmull.u8	q10, d3, d2
	vrshr.u16	q11, q8, #8
	vswp	d3, d31
	vrshr.u16	q12, q9, #8
	vrshr.u16	q13, q10, #8
	vraddhn.i16	d30, q11, q8
	vraddhn.i16	d29, q12, q9
	vraddhn.i16	d28, q13, q10
	vst4.8	{d28-d31}, [r2]!
16:	tst	r0, #7
	beq	17f
	tst	r0, #4
	beq	18f
	vld1.32	{d2-d3}, [r4]!
18:	tst	r0, #2
	beq	19f
	vld1.32	{d1}, [r4]!
19:	tst	r0, #1
	beq	20f
	vld1.32	{d0[1]}, [r4]!
20:	vuzp.8	d0, d1
	vuzp.8	d2, d3
	vuzp.8	d1, d3
	vuzp.8	d0, d2
	vmull.u8	q8, d3, d0
	vmull.u8	q9, d3, d1
	vmull.u8	q10, d3, d2
	vrshr.u16	q11, q8, #8
	vswp	d3, d31
	vrshr.u16	q12, q9, #8
	vrshr.u16	q13, q10, #8
	vraddhn.i16	d30, q11, q8
	vraddhn.i16	d29, q12, q9
	vraddhn.i16	d28, q13, q10
	vzip.8	d28, d30
	vzip.8	d29, d31
	vzip.8	d30, d31
	vzip.8	d28, d29
	tst	r0, #4
	beq	21f
	vst1.32	{d30-d31}, [r2]!
21:	tst	r0, #2
	beq	22f
	vst1.32	{d29}, [r2]!
22:	tst	r0, #1
	beq	17f
	vst1.32	{d28[1]}, [r2]!
17:	mov	r0, r7
	add	r2, r2, r3, lsl #2
	add	r4, r4, r5, lsl #2
	sub	r2, r2, r0, lsl #2
	sub	r4, r4, r0, lsl #2
	subs	r1, r1, #1
	mov	r6, r2
	bge	1b
0:	pop	{r4, r5, r6, r7, r8, r9, sl, fp, ip, pc}

#endif
